<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>DoBetter</title>
  <meta name="author" content="DoBetter">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="assets/kitty.png" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Dianbing Xi
                  </p>

                  <p>
                    I'm a final-year Ph.D. student in the 
                    <a href="https://www.rendering.group/">Advanced Rendering and Creativity (ARC) Group</a> 
                    at the <a href="http://www.cad.zju.edu.cn/">State Key Laboratory of CAD&CG</a>, 
                    <a href="https://www.zju.edu.cn/">Zhejiang University</a>, 
                    supervised by Prof. <a href="http://www.cad.zju.edu.cn/home/rwang/">Rui Wang</a> and 
                    Prof. <a href="http://www.cad.zju.edu.cn/home/huo/">Yuchi Huo</a>.
                    I have been fortunate to collaborate with Dr. 
                    <a href="https://jiepengwang.github.io/">Jiepeng Wang</a> at TeleAI, 
                    Prof. <a href="https://liuyuan-pal.github.io/">Yuan Liu</a> at the Hong Kong University of Science and Technology, 
                    and Prof. <a href="https://xiuyuliang.cn/">Yuliang Xiu</a> at Westlake University.
                  </p>
                  


                  <p style="text-align:center">
                    <a href="mailto:db.xi@zju.edu.cn">Email</a> &nbsp;/&nbsp;
                    <a href="https://www.overleaf.com/read/ncnbkgfgjpcr#bc0579">CV</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=7H29mf4AAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                    <a href="https://github.com/xdobetter">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:37%;max-width:37%">
                  <a href="assets/me.jpg"><img
                      style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo"
                      src="assets/me.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                  <p>
                    My research primarily focuses on
                    <strong>Computer Graphics</strong> and
                    <strong>Artificial Intelligence Generated Content (AIGC)</strong>.
                    I currently work on <strong>controllable video generation</strong>,
                    which I see as a promising new paradigm for future rendering.
                    I also have prior experience in
                    <strong>3D avatar generation</strong>,
                    <strong>inverse rendering</strong>, and
                    <strong>neural rendering</strong>.
                  </p>
                  

                  <p style="text-decoration: underline;">
                    <span style="color: red; font-size: 1.4em; font-weight: bold;">!!!</span>
                    I am expected to complete my Ph.D. in
                    <strong><span class="highlight">June 2026</span></strong> and am actively seeking
                    <strong><span class="highlight">research positions in industry</span></strong> or
                    <strong><span class="highlight">postdoctoral opportunities</span></strong>.
                  </p>
                  <p>
                    Feel free to contact me at <strong>db.xi@zju.edu.cn</strong> regarding relevant positions or collaboration opportunities.
                  </p>
             
                  
                </td>
              </tr>
            </tbody>
          </table>



         
          <table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;">
            <tbody>
              <tr>
                <td>
                  <h2>Selected Publications</h2>
                </td>
              </tr>
            </tbody>
          </table>
          



          <table
            style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td style="padding:16px;width:30%;vertical-align:middle">
                  <img src="./assets/ctrlvdiff_teaser.png" width="100%">
                </td>

                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://tele-ai.github.io/CtrlVDiff/">
                    <span class="papertitle">CtrlVDiff: Controllable Video Generation via Unified Multimodal Video
                      Diffusion</span>
                  </a>
                  <br>
                  <strong>Dianbing Xi</strong>,
                  <a href="https://jiepengwang.github.io/">Jiepeng Wang</a>,
                  <a href="https://akira-l.github.io/">Yuanzhi Liang</a>,
                  <a href="">Xi Qiu</a>,
                  <a href="">Jialun Liu</a>,
                  <a href="https://haopan.github.io/">Hao Pan</a>,
                  <a href="http://www.cad.zju.edu.cn/home/huo/">Yuchi Huo</a>,
                  <a href="http://www.cad.zju.edu.cn/home/rwang/">Rui Wang</a>,
                  <a href="https://brotherhuang.github.io/index.html">Haibin Huang</a>,
                  <a href="https://scholar.google.com/citations?hl=en&user=PXlNTokAAAAJ">Chi Zhang</a>,
                  <a href="http://xuelongli.cn/en.php">Xuelong Li</a>
                  <br>
                  <em>arXiv</em>, 2025
                  <br>
                  <a href="https://tele-ai.github.io/CtrlVDiff/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2511.21129">arXiv</a>
                  <p></p>
                  <p>
                    CtrlVDiff unifies forward and inverse video generation within a single model, enabling the
                    extraction of all modalities in a single pass. It provides layer-wise control over appearance and
                    structure, facilitating applications such as material editing and object insertion.
                  </p>
                </td>
              </tr>


              <tr>
                <td style="padding:16px;width:30%;vertical-align:middle">
                  <img src="./assets/omnivdiff_teaser.png" width="100%">
                </td>

                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://tele-ai.github.io/OmniVDiff/">
                    <span class="papertitle">OmniVDiff: Omni Controllable Video Diffusion for Generation and Understanding</span>
                  </a>
                  <br>
                  <strong>Dianbing Xi</strong>,
                  <a href="https://jiepengwang.github.io/">Jiepeng Wang</a>,
                  <a href="https://akira-l.github.io/">Yuanzhi Liang</a>,
                  <a href="">Xi Qiu</a>,
                  <a href="http://www.cad.zju.edu.cn/home/huo/">Yuchi Huo</a>,
                  <a href="http://www.cad.zju.edu.cn/home/rwang/">Rui Wang</a>,
                  <a href="https://scholar.google.com/citations?hl=en&user=PXlNTokAAAAJ">Chi Zhang</a>,
                  <a href="http://xuelongli.cn/en.php">Xuelong Li</a>
                  <br>
                  <em>AAAI</em>, 2026
                  <br>
                  <a href="https://tele-ai.github.io/OmniVDiff/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2504.10825">arXiv</a>
                  <p>
                    OmniVDiff enables controllable video generation and understanding in a unified video diffusion framework.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:16px;width:30%;vertical-align:middle">
                  <img src="./assets/pfavatar_teaser.png" width="100%">
                </td>

                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://xdobetter.github.io/OOTD_PFAvatar/">
                    <span class="papertitle">PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos</span>
                  </a>
                  <br>
                  <strong>Dianbing Xi</strong>,
                  <a href="https://anguoyuan.github.io/">Guoyuan An</a>,
                  <a href="https://jingsenzhu.github.io/">Jingsen Zhu</a>,
                  <a href="">Zhijian Liu</a>,
                  <a href="https://liuyuan-pal.github.io/">Yuan Liu</a>,
                  <a href="https://scholar.google.com/citations?user=Vw5X8yEAAAAJ&hl=en">Ruiyuan Zhang</a>,
                  <a href="https://github.com/jiayuanlu">Jiayuan Lu</a>,
                  <a href="http://www.cad.zju.edu.cn/home/huo/">Yuchi Huo</a>,
                  <a href="http://www.cad.zju.edu.cn/home/rwang/">Rui Wang</a>
                  <br>
                  <em>AAAI</em>, 2026
                  <br>
                  <a href="https://xdobetter.github.io/OOTD_PFAvatar/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2511.12935">arXiv</a>
                  <p>
                    PFAvatar reconstructs and edits personalized avatars from OOTD photos using pose-aware diffusion
models and 3D-SDS, overcoming previous method limitations and supporting editing and animation.
                  </p>
                </td>
              </tr>


              <tr>
                <td style="padding:16px;width:30%;vertical-align:middle">
                  <img src="./assets/anitex_teaser.webp" width="100%">
                </td>

                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="">
                    <span class="papertitle">AniTex: light-geometry consistent PBR Material Generation for Animatable Objects</span>
                  </a>
                  <br>
                  <a href="">Jieting Xu</a>,
                  <a href="">Ziyi Xu</a>,
                  <a href="https://yiweihu.netlify.app/">Yiwei Hu</a>,
                  <a href="https://anguoyuan.github.io/">Guoyuan An</a>,
                  <a href="https://rgxie.me/">Rengan Xie</a>,
                  <a href="">Zhijian Liu</a>,
                  <strong>Dianbing Xi</strong>,
                  <a href="">Wenjun Song</a>,                  
                  <a href="http://www.cad.zju.edu.cn/home/huo/">Yuchi Huo</a>,
                  <a href="http://www.cad.zju.edu.cn/home/rwang/">Rui Wang</a>
                  <br>
                  <em>SIGGRAPH ASIA</em>, 2025
                  <br>
                  <a  href="">project page</a> /
                  <a  href="">arXiv</a>
                  <p>
                    AniTex generates high-quality, temporally consistent PBR materials for animated 3D objects using a two-stage diffusion pipeline, achieving realistic multi-view textures and outperforming existing static-focused methods.
                  </p>
                </td>
              </tr>


              <tr>
                <td style="padding:16px;width:30%;vertical-align:middle">
                  <img src="./assets/teaser_iccv.png" width="100%">
                </td>

                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://intrinsiccontrolnet.github.io/">
                    <span class="papertitle">IntrinsicControlNet: Cross-distribution Image Generation with Real and Unreal</span>
                  </a>
                  <br>
                  <a href="https://github.com/jiayuanlu">Jiayuan Lu</a>,
                  <a href="https://rgxie.me/">Rengan Xie</a>,
                  <a href="">Zixuan Xie</a>,
                  <a href="">Zhizhen Wu</a>,
                  <strong>Dianbing Xi</strong>,
                  <a href="https://perple-zju.github.io/">Qi Ye</a>,        
                  <a href="http://www.cad.zju.edu.cn/home/rwang/">Rui Wang</a>,
                  <a href="http://www.cad.zju.edu.cn/home/bao/">Hujun Bao</a>,
                  <a href="http://www.cad.zju.edu.cn/home/huo/">Yuchi Huo</a>
                  <br>
                  <em>ICCV</em>, 2025
                  <br>
                  <a  href="https://intrinsiccontrolnet.github.io/">project page</a> /
                  <a  href="">arXiv</a>
                  <p>
                    IntrinsicControlNet uses intrinsic images and cross-domain control to generate photorealistic yet explicitly controllable images, combining rendering-level precision with diffusion-model realism while bridging gaps between synthetic and real data.
                  </p>
                </td>
              </tr>


              <tr>
                <td style="padding:16px;width:30%;vertical-align:middle">
                  <img src="./assets/dualband_teaser.png" width="100%">
                </td>

                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://mshnb.github.io/dualbandfusion/">
                    <span class="papertitle">Dual-Band Feature Fusion for All-Frequency Neural Global Illumination</span>
                  </a>
                  <br>
                  <a href="">Shaohua Mo</a>,
                  <a href="">Chuankun Zheng</a>,
                  <a href="">Zihao Lin</a>,
                  <strong>Dianbing Xi</strong>,
                  <a href="https://perple-zju.github.io/">Qi Ye</a>,        
                  <a href="http://www.cad.zju.edu.cn/home/rwang/">Rui Wang</a>,
                  <a href="http://www.cad.zju.edu.cn/home/bao/">Hujun Bao</a>,
                  <a href="http://www.cad.zju.edu.cn/home/huo/">Yuchi Huo</a>
                  <br>
                  <em>SIGGRAPH</em>, 2025
                  <br>
                  <a  href="https://mshnb.github.io/dualbandfusion/">project page</a> /
                  <a  href="">arXiv</a>
                  <p>
                    We propose a dual-band neural GI framework using object-centric feature grids and single-bounce queries to fuse low- and high-frequency illumination, enabling high-quality multi-frequency dynamic reflections beyond prior GI and denoising methods.
                  </p>
                </td>
              </tr>

          


              <tr>
                <td style="padding:16px;width:30%;vertical-align:middle">
                  <img src="./assets/mirres_teaser.png" width="100%">
                </td>

                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://brabbitdousha.github.io/MIRReS/">
                    <span class="papertitle">MIRReS: Inverse Rendering using Multi-Bounce Path Tracing and Reservoir Sampling</span>
                  </a>
                  <br>
                  <a href="https://brabbitdousha.github.io/">Yuxin Dai</a>,
                  <a href="https://cce.ncepu.edu.cn/szdw/jsml/jsjkxyjsjys/682c2df08a1247e3bdd6b3d6ec19ea48.htm">Qi Wang</a>,
                  <a href="https://jingsenzhu.github.io/">Jingsen Zhu</a>,
                  <strong>Dianbing Xi</strong>,
                  <a href="http://www.cad.zju.edu.cn/home/huo/">Yuchi Huo</a>
                  <a href="">Chen Qian</a>,        
                  <a href="https://personal.ntu.edu.sg/yhe/">Ying He</a>

                  <br>
                  <em>ICLR</em>, 2025
                  <br>
                  <a  href="https://brabbitdousha.github.io/MIRReS/">project page</a> /
                  <a  href="https://arxiv.org/abs/2406.16360">arXiv</a>
                  <p>
                    We propose MIRReS, a two-stage inverse rendering framework that recovers explicit geometry, materials, and lighting using multi-bounce path tracing with reservoir-sampled gradients, achieving accurate intrinsic decomposition and SOTA performance.
                  </p>
                </td>
              </tr>
              
           


              <tr>
                <td style="padding:16px;width:30%;vertical-align:middle">
                  <img src="./assets/I2_teaser.png" width="100%">
                </td>

                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://jingsenzhu.github.io/i2-sdf/">
              I^2-SDF: Intrinsic Indoor Scene Reconstruction and Editing via Raytracing in Neural SDFs
              <span class="papertitle"></span>
                  </a>
                  <br>
                  <a href="https://jingsenzhu.github.io/">Jingsen Zhu</a>,
                  <a href="http://www.cad.zju.edu.cn/home/huo/">Yuchi Huo</a>,
                  <a href="https://perple-zju.github.io/">Qi Ye</a>,        
                  <a href="">Fujun Luan</a>,
                  <a href="">Jifan Li</a>,
                  <strong>Dianbing Xi</strong>,    
                  <a href="">Lisha Wang</a>,
                  <a href="">Rui Tang</a>,  
                  <a href="">Wei Hua</a>,  
                  <a href="http://www.cad.zju.edu.cn/home/bao/">Hujun Bao</a>,
                  <a href="http://www.cad.zju.edu.cn/home/rwang/">Rui Wang</a>
                  <br>
                  <em>CVPR</em>, 2023
                  <br>
                  <a  href="https://jingsenzhu.github.io/i2-sdf/">project page</a> /
                  <a  href="https://arxiv.org/abs/2303.07634">arXiv</a>
                  <p>
                    We introduce I^2-SDF, a neural SDF–based inverse rendering framework that reconstructs indoor geometry, materials, and radiance via differentiable MC ray tracing, enabling high-quality reconstruction, relighting, and editing with SOTA performance.
                  </p>
                </td>
              </tr>

         

              <tr>
                <td style="padding:16px;width:30%;vertical-align:middle">
                  <img src="./assets/invrender_teaser.png" width="100%">
                </td>

                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://jingsenzhu.github.io/invrend/">
                    Learning-based Inverse Rendering of Complex Indoor Scenes with Differentiable Monte Carlo Raytracing
              <span class="papertitle"></span>
                  </a>
                  <br>
                  <a href="https://jingsenzhu.github.io/">Jingsen Zhu</a>,
                  <a href="">Fujun Luan</a>,
                  <a href="http://www.cad.zju.edu.cn/home/huo/">Yuchi Huo</a>,       
                  <a href="">Zihao Lin</a>,
                  <a href="https://isaac-paradox.github.io/">Zhihua Zhong</a>,
                  <strong>Dianbing Xi</strong>,    
                  <a href="">Jiaxiang Zheng</a>,
                  <a href="">Rui Tang</a>,   
                  <a href="http://www.cad.zju.edu.cn/home/rwang/">Rui Wang</a>,
                  <a href="http://www.cad.zju.edu.cn/home/bao/">Hujun Bao</a>
                  <br>
                  <em>SIGGRAPH Asia</em>, 2022
                  <br>
                  <a  href="https://jingsenzhu.github.io/invrend/">project page</a> /
                  <a  href="https://arxiv.org/abs/2211.03017">arXiv</a>
                  <p>
                    We propose a single-image inverse rendering framework using differentiable MC ray tracing and uncertainty-aware out-of-view lighting prediction, enabling accurate geometry, lighting, and material recovery with photorealistic edits beyond prior methods.
                  </p>
                </td>
              </tr>
              

              <table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;">
                <tbody>
                  <tr>
                    <td>
                      <h2>Experience</h2>
                    </td>
                  </tr>
                </tbody>
              </table>
             
              
              <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin:0 auto;">
                <tbody>
                  <tr>
                    <td style="padding:0;">
                      <div style="text-align:center; margin-top:30px;">
                        <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=-aJTY8q0U71ICtskpMPuzBIO9U3yS16_m09hpVOYoEo&cl=ffffff&w=300"></script>
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td style="padding:0;">
                      <br>
                      <p style="text-align:center; font-size:small; color:#000;">
                        ©<strong>Dianbing Xi</strong>.  
                        Last updated: <span id="last-updated"></span>.
                      </p>
                    </td>
                  </tr>
                </tbody>
              </table>
 
              

        </td>
      </tr>
  </table>



  <script>
    fetch(window.location.href, { method: 'HEAD' })
      .then(response => {
        const lastModified = response.headers.get('Last-Modified');
        if (lastModified) {
          const date = new Date(lastModified);
  
          const day = date.getDate();
          const monthNames = [
            "January", "February", "March", "April", "May", "June",
            "July", "August", "September", "October", "November", "December"
          ];
          const month = monthNames[date.getMonth()];
          const year = date.getFullYear();
  
          document.getElementById("last-updated").textContent =
            `${day}th ${month}, ${year}`;
        }
      })
      .catch(err => {
        console.log("Failed to load Last-Modified:", err);
      });
  </script>
  


  <script>
    document.querySelectorAll('a').forEach(a => {
      const href = a.getAttribute('href');
      // 空字符串、#、null 都视为 “无效链接”
      if (!href || href.trim() === "" || href.trim() === "#" ) {
        a.style.pointerEvents = "none";    // 禁止点击
        a.style.color = "gray";            // 变灰色
        a.style.textDecoration = "none";   // 去掉下划线
        a.style.cursor = "default";        // 鼠标变回普通箭头
      }
    });
  </script>




</body>

</html>